---
title: "HDT6-Modelo de regresion logistica"
author: "Grupo9"
date: "2023-04-14"
output: html_document
---


```{r setup, include=FALSE} # nolint
knitr::opts_chunk$set(echo = TRUE)
```

## Hoja de trabajo 6: Modelos de regresion logistica

```{r message=FALSE, warning=FALSE, include=TRUE, paged.print=FALSE}
#Librerías a utilizar
#install.packages("dummies")  # nolint
library(rmarkdown)
library(ModelMetrics)
library(ggplot2)
library(caret)
library(GGally)
library(modelr)
```


#### 1.Cree una variable dicotómica por cada una de las categorías de la variable respuesta categórica que creó en hojas anteriores. Debería tener 3 variables dicotómicas (valores 0 y 1) una que diga si la vivienda es cara o no, media o no, económica o no.
```{r message=FALSE, warning=FALSE, paged.print=FALSE}
#Cargamos y leemos la data
data <- read.csv('train.csv') # nolint
data[is.na(data)] <- 0

#Calculo de percentiles
percentil <- quantile(data$SalePrice)

#Creamos la variable dicotómica "Estado"
data$Estado <- ifelse(data$SalePrice <= 129975, "Economica",
                ifelse(data$SalePrice > 129975 & data$SalePrice <= 163000, "Intermedia", "Cara")) # nolint

#Modelo de Regresion logistica
porcentaje <- 0.7
datos <- data

#Experimento reproducible
set.seed(123)

#Variables dicotomicas
datos$Economica <- as.numeric(datos$Estado == "Economica")
datos$Intermedia <- as.numeric(datos$Estado == "Intermedia")
datos$Cara <- as.numeric(datos$Estado == "Cara")

head(datos, n = 3)
```

#### 2.Use los mismos conjuntos de entrenamiento y prueba que utilizó en las hojas anteriores.
```{r}
#Utilizamos mismos conjutnos
corte <- sample(nrow(datos),nrow(datos)*porcentaje)
train<-datos[corte,]
test<-datos[-corte,]
```

#### 3.Primer modelo

```{r warning=FALSE}
#Nos interesa saber si una casa con alto valor o no 
modelo<-glm(Cara~., data = train[,c('SalePrice','GrLivArea','Cara','LotFrontage','LotArea','BsmtQual','PoolArea')],family = binomial(), maxit=100)
modelo
```
Para poder conocer si la casa es de alto valor o no (es decir si es cara) hacemos uso de 7 variables que nos ayudan con el objetivo. Estas son:'SalePrice','LotArea','BsmtQual','GrLivArea','Cara','PoolArea','LotFrontage'.

#### 4. Analice el modelo. Determine si hay multicolinealidad en las variables, y cuáles son las que aportan al modelo, por su valor de significación. Haga un análisis de correlación de las variables del modelo y especifique si el modelo se adapta bien a los datos. 
```{r message=FALSE, warning=FALSE}
#Analisis de correlacion de las variables mencionadas
ggpairs(datos[,c('SalePrice','GrLivArea','LotFrontage','LotArea','BsmtQual','PoolArea')])
```
Como es posible notar, la gráfica muestra que la mayoria de las vaiables que hemos utilizado si tienene una buena correlación. La excepción que observamos es con las varibales BsmQual y PoolArea, ya que estos si muestran que su correlación no es la mejor. Este caso nos hace reconsiderar si son servibles para el análisis de predicción o no. 

#### 5.
```{r message=FALSE, warning=FALSE, include=TRUE, paged.print=FALSE}
library(car)

library(caret)


# Define the resampling method
train_control <- trainControl(method = "cv", number = 10)

# Fit the logistic regression model using 10-fold cross-validation
model <- train(Cara ~ ., data = train[, c('SalePrice','GrLivArea','Cara','LotFrontage','LotArea','BsmtQual','PoolArea')],
               method = "glm",
               trControl = train_control,
               family = binomial())
model



# print the model summary
summary(model)


# Analisis de multicolinealidad
vif(modelo)


```
```{r}
# Realizamos predicciones en el conjunto de prueba
predicciones <- predict(modelo, newdata = test[, c('SalePrice','GrLivArea','Cara','LotFrontage','LotArea','BsmtQual','PoolArea')], type = "response")

# Convertimos las predicciones a clasificaciones binarias
predicciones_binarias <- ifelse(predicciones > 0.5, 1, 0)

# Calculamos la precisión del modelo en el conjunto de prueba
precision <- mean(predicciones_binarias == test$Cara)

# Mostramos la precisión
precision
```





#### 10. Modelo de árbol de decisión

```{r}
#Modelo de árbol de decisión
library(rpart)
library(rpart.plot)

arbol <- rpart(Cara ~., data=train[,c('SalePrice','GrLivArea','Cara','LotFrontage','LotArea','BsmtQual','PoolArea')], method="class")
prp(arbol)

#Modelo de Random Forest
library(randomForest)

forest <- randomForest(Cara ~., data=train[,c('SalePrice','GrLivArea','Cara','LotFrontage','LotArea','BsmtQual','PoolArea')], importance=TRUE)
print(forest)

#Modelo de Naive Bayes
library(e1071)

naive <- naiveBayes(Cara ~., data=train[,c('SalePrice','GrLivArea','Cara','LotFrontage','LotArea','BsmtQual','PoolArea')], laplace=1)
print(naive)


```


#### 11. El modelo de Naive Bayes nos muestra la probabilidad de que una casa sea cara o no, dadas las diferentes variables que se tienen en cuenta. En este caso, las variables que más influyen en la predicción son la calidad del sótano (BsmtQual), el área de la casa (GrLivArea) y el precio de venta (SalePrice).

En conclusión, los cuatro modelos (Regresión Logística, Árbol de Decisión, Random Forest y Naive Bayes) utilizan las mismas variables para predecir si una casa es cara o no. Cada modelo ofrece información valiosa acerca de las variables más importantes en la predicción. En general, se puede observar que el precio de venta, el área de la casa y la calidad del sótano son variables importantes en todos los modelos.

